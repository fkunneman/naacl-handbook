SubmissionNumber#=%=#6
FinalPaperTitle#=%=#Distilling Word Embeddings: An Encoding Approach
ShortPaperTitle#=%=#Distilling Word Embeddings: An Encoding Approach
NumberOfPages#=%=#5
CopyrightSigned#=%=#NA
JobTitle#==#
Organization#==#
Abstract#==#Distilling knowledge from a well-trained cumbersome network to a small one has
recently become a new research topic, as lightweight neural networks with high
performance are particularly in need in various resource-restricted systems.
This paper addresses the problem of distilling word embeddings for NLP tasks.
We propose an encoding approach to distill task-specific knowledge from
high-dimensional embeddings, which can reduce model complexity by a large
margin. Experiments in two tasks reveal the phenomenon that distilling
knowledge from cumbersome embeddings is better than directly training neural
networks with small embeddings.
Author{1}{Firstname}#=%=#Lili
Author{1}{Lastname}#=%=#Mou
Author{1}{Email}#=%=#doublepower.mou@gmail.com
Author{1}{Affiliation}#=%=#Peking University
Author{2}{Firstname}#=%=#Ran
Author{2}{Lastname}#=%=#Jia
Author{2}{Email}#=%=#jiaran1994@sina.com
Author{2}{Affiliation}#=%=#Peking University at Beijing
Author{3}{Firstname}#=%=#Yan
Author{3}{Lastname}#=%=#Xu
Author{3}{Email}#=%=#xuyan14@pku.edu.cn
Author{3}{Affiliation}#=%=#PKU
Author{4}{Firstname}#=%=#Ge
Author{4}{Lastname}#=%=#Li
Author{4}{Email}#=%=#lige@sei.pku.edu.cn
Author{4}{Affiliation}#=%=#Peking University
Author{5}{Firstname}#=%=#Lu
Author{5}{Lastname}#=%=#Zhang
Author{5}{Email}#=%=#zhanglu@sei.pku.edu.cn
Author{5}{Affiliation}#=%=#Peking University
Author{6}{Firstname}#=%=#Zhi
Author{6}{Lastname}#=%=#Jin
Author{6}{Email}#=%=#zhijin@sei.pku.edu.cn
Author{6}{Affiliation}#=%=#Peking University

==========