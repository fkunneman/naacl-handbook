SubmissionNumber#=%=#91
FinalPaperTitle#=%=#Compression of Neural Machine Translation Models via Pruning
ShortPaperTitle#=%=#Compression of NMT Models via Pruning
NumberOfPages#=%=#11
CopyrightSigned#=%=#Abigail See
JobTitle#==#student
Organization#==#Computer Science Department, Stanford University, Stanford, CA 94305
Abstract#==#Neural Machine Translation (NMT), like many other deep learning domains,
typically suffers from over-parameterization, resulting in large storage sizes.
This paper examines three simple magnitude-based pruning schemes to compress
NMT models, namely
class-blind, class-uniform and class-distribution, which differ in terms of how
pruning thresholds are computed for the different classes of weights in the NMT
architecture. We demonstrate the efficacy of weight pruning as a compression
technique for a state-of-the-art NMT system. We show that an NMT
model with over 200 million parameters can be pruned by 40% with very little
performance loss as measured on the WMT'14 English-German translation task.
This sheds light on the distribution of redundancy in the NMT architecture. 
Our main result is that with with retraining, we can recover and even surpass
the original performance with an 80%-pruned model.
Author{1}{Firstname}#=%=#Abigail
Author{1}{Lastname}#=%=#See
Author{1}{Email}#=%=#abisee@stanford.edu
Author{1}{Affiliation}#=%=#Stanford University
Author{2}{Firstname}#=%=#Minh-Thang
Author{2}{Lastname}#=%=#Luong
Author{2}{Email}#=%=#luong.m.thang@gmail.com
Author{2}{Affiliation}#=%=#Stanford University
Author{3}{Firstname}#=%=#Christopher D.
Author{3}{Lastname}#=%=#Manning
Author{3}{Email}#=%=#manning@cs.stanford.edu
Author{3}{Affiliation}#=%=#Stanford University

==========