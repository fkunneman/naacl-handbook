SubmissionNumber#=%=#135
FinalPaperTitle#=%=#Results of the WMT16 Metrics Shared Task
ShortPaperTitle#=%=#Results of the WMT16 Metrics Shared Task
NumberOfPages#=%=#33
CopyrightSigned#=%=#XXX
JobTitle#==#
Organization#==#XXX
Abstract#==#This paper presents the results of the WMT16 Metrics Shared Task. We asked
participants of this task to score the outputs of the MT systems involved in
the WMT16 Shared Translation Task. We collected scores of 16 metrics from 9
research groups. In addition to that, we computed scores of 9 standard metrics
(BLEU, SentBLEU, NIST, WER, PER, TER and CDER) as baselines. The collected
scores were evaluated in terms of system-level correlation (how well each
metric's scores correlate with WMT16 official manual ranking of systems) and in
terms of segment level correlation (how often a metric agrees with humans in
comparing two translations of a particular sentence).

This year there are several additions to the setup: large number of language
pairs (18 in total),datasets from different domains (news, IT and medical), and
different kinds of judgments: relative ranking (RR), direct assessment (DA) and
HUME manual semantic judgments. Finally, generation of large number of hybrid
systems was trialed for provision of more conclusive system-level metric
rankings.
Author{1}{Firstname}#=%=#Ondřej
Author{1}{Lastname}#=%=#Bojar
Author{1}{Email}#=%=#bojar@ufal.mff.cuni.cz
Author{1}{Affiliation}#=%=#Charles University in Prague, Faculty of Mathematics and Physics
Author{2}{Firstname}#=%=#Yvette
Author{2}{Lastname}#=%=#Graham
Author{2}{Email}#=%=#graham.yvette@gmail.com
Author{2}{Affiliation}#=%=#Dublin City University
Author{3}{Firstname}#=%=#Amir
Author{3}{Lastname}#=%=#Kamran
Author{3}{Email}#=%=#a.kamran@uva.nl
Author{3}{Affiliation}#=%=#University of Amsterdam
Author{4}{Firstname}#=%=#Miloš
Author{4}{Lastname}#=%=#Stanojević
Author{4}{Email}#=%=#milosh.stanojevic@gmail.com
Author{4}{Affiliation}#=%=#University of Amsterdam, ILLC

==========