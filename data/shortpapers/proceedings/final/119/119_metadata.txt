SubmissionNumber#=%=#119
FinalPaperTitle#=%=#A Latent Concept Topic Model for Robust Topic Inference Using Word Embeddings
ShortPaperTitle#=%=#A Latent Concept Topic Model for Robust Topic Inference Using Word Embeddings
NumberOfPages#=%=#7
CopyrightSigned#=%=#Weihua Hu
JobTitle#==#author
Organization#==#University of Tokyo
The University of Tokyo 7-3-1 Hongo Bunkyo-Ku, Tokyo, Japan
Abstract#==#Uncovering thematic structures of SNS and blog posts is a crucial yet
challenging task, because of the severe data sparsity induced by the short
length of texts and diverse use of vocabulary. This hinders effective topic
inference of traditional LDA because it infers topics based on document-level
co-occurrence of words. To robustly infer topics in such contexts, we propose a
latent concept topic model (LCTM). Unlike LDA, LCTM reveals topics via
co-occurrence of latent concepts, which we introduce as latent variables to
capture conceptual similarity of words. More specifically, LCTM models each
topic as a distribution over the latent concepts, where each latent concept is
a localized Gaussian distribution over the word embedding space. Since the
number of unique concepts in a corpus is often much smaller than the number of
unique words, LCTM is less susceptible to the data sparsity. Experiments on the
20Newsgroups show the effectiveness of LCTM in dealing with short texts as well
as the capability of the model in handling held-out documents with a high
degree of OOV words.
Author{1}{Firstname}#=%=#Weihua
Author{1}{Lastname}#=%=#Hu
Author{1}{Email}#=%=#weihua916@gmail.com
Author{1}{Affiliation}#=%=#The University of Tokyo
Author{2}{Firstname}#=%=#Jun'ichi
Author{2}{Lastname}#=%=#Tsujii
Author{2}{Email}#=%=#j-tsujii@aist.go.jp
Author{2}{Affiliation}#=%=#Aritificial Intelligence Research Centre at AIST

==========