\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
\usepackage{caption}
\usepackage{subcaption}


% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage{acl2016} 
\aclfinalcopy

\usepackage{times}
\usepackage{latexsym}

\usepackage{flushend}

\usepackage{todonotes}

\newcommand{\gcca}{\emph{GCCA}} % Weighted GCCA over views
\newcommand{\gccasv}{\emph{GCCA-sv}} % Weight the GCCA features by the singular values of \tilde{M}, where the singular values X_i are not scaled to 1
\newcommand{\gccasvandbow}{\emph{GCCA-sv + BOW}}
\newcommand{\gccaandbow}{\emph{GCCA + BOW}}
\newcommand{\gccawnet}{\emph{GCCA-net}} % Weighted GCCA over views, including friend/follower networks as two additional views
\newcommand{\bow}{\emph{BOW}} % Raw bag-of-words
\newcommand{\bowpca}{\emph{BOW-PCA}} % PCA over views
\newcommand{\bowpcawnet}{\emph{NetSim-PCA}} % PCA over network
\newcommand{\bowpcaandbow}{\emph{BOW-PCA + BOW}}
\newcommand{\wordtovec}{\emph{Word2Vec}}
\newcommand{\collab}{\emph{NetSim}} % Network similarity
\newcommand{\rand}{\emph{Random}} % Random ranking baseline
\newcommand{\tweetrate}{\emph{TweetRate}} % Users who tend to tweet a lot are ranked higher
\newcommand{\netpop}{\emph{NetSize}} % Users who have a lot of edges are ranked higher

\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}

\newcommand{\myparagraph}[1]{\vspace{-.2cm}\paragraph{#1}}

\usepackage[font=small]{caption}

\begin{document} 

\title{Learning Multiview Embeddings of Twitter Users}


\author{Adrian Benton$^*$ \\
  {\tt adrian@cs.jhu.edu} \\
  \\
  \\ \And
  Raman Arora$^*$ \\
  {\tt arora@cs.jhu.edu} \\
  $^*$Human Language Technology Center of Excellence\\
   Center for Language and Speech Processing, Johns Hopkins University\\
   Baltimore, MD 21218 USA \\
   $^{\dagger}$Bloomberg LP, New York, NY 10022\\
   \And
  Mark Dredze$^{* \dagger}$ \\
  {\tt mdredze@cs.jhu.edu}}


\date{}


\maketitle

% \vspace*{-20pt}

\begin{abstract}
Low-dimensional vector representations are widely used as stand-ins for the text of words, sentences, and entire documents.
These embeddings are used to identify similar words or make predictions about documents.
In this work, we consider embeddings for social media users and demonstrate that these
can be used to identify users who behave similarly or to predict attributes of users.
In order to capture information from all aspects of a user's online life, we take a multiview approach,
applying a weighted variant of Generalized Canonical Correlation Analysis
(GCCA) to a collection of over 100,000 Twitter users.
We demonstrate the utility of these multiview embeddings on three downstream tasks: user engagement, friend selection, and 
demographic attribute prediction.
\end{abstract}

%\vspace*{-5pt}
\section{Introduction}
\label{background}
%\vspace*{-5pt}

Dense, low-dimensional vector representations (embeddings) have a long history in NLP, and recent work on neural models have provided new and 
popular algorithms for training representations for word types \cite{mikolov2013,faruqui2014},
sentences \cite{kiros2015}, and entire documents \cite{le2014}.  These embeddings often have nice properties, such
as capturing some aspects of syntax or semantics and outperforming their sparse counterparts at downstream tasks.

While there
are many approaches to generating embeddings of {\em text}, it is not clear how to learn embeddings for social media {\em users}.
There are several different types of data (views) we can use to build user representations: the text of messages they post,
neighbors in their local network, articles they link to, images they upload, etc. We propose unsupervised learning of representations
of users with a variant of Generalized Canonical Correlation Analysis (\gcca) \cite{carroll1968generalization,van2006,arora2014multi,rastogi2015}, a multiview
technique that learns a single, low-dimensional vector for each user best capturing information from each of their views.  We believe
this is more appropriate for learning user embeddings than concatenating views into a single vector, since 
views may correspond to different modalities (image vs. text data) or have very different distributional properties.  Treating
all features as equal in this concatenated vector is not appropriate.

We offer two main contributions: (1) an application of \gcca{} to learning vector representations of social media users
that best accounts for all aspects of a user's online life, and (2) an evaluation of these vector
representations for a set of Twitter users at three different tasks: user engagement, friend, and demographic
attribute prediction.


%\vspace*{-5pt}
\section{Twitter User Data}
\label{sec:data}
%\vspace*{-5pt}
We begin with a description of our dataset, which is necessary for understanding the data available to our multiview model.
We uniformly sampled 200,000 users from a stream of publicly available tweets from the 1\% Twitter stream from April 2015.
To include typical, English speaking users we removed users with verified accounts, more than 10,000 followers, or non-English
profiles\footnote{Identified with \texttt{langid} \cite{lui2012langid}.}.
For each user we collected their 1,000 most recent tweets, and then filtered out non-English tweets.
Users without English tweets in January or February 2015 were omitted, yielding a total of 102,328 users.
Although limiting tweets to only these two months restricted the number of tweets we were able to work with, it also ensured that our data are drawn from a narrow time window, controlling for differences in user activity over time. This allows us to learn 
distinctions between users, and not temporal distinctions of content.
We will use this set of users to learn representations for the remainder of this paper. 

Next, we expand the information available about these users by collecting information about their social networks.
Specifically, for each user mentioned in a tweet by one of the 102,328 users, we collect up to the 200 most recent English tweets for these users from January and February 2015. Similarly, we collected the 5,000 most recently added friends and followers of each of the 102,328 users. We then sampled 10 friends and 10 followers for each user and collected up to the 200 most recent English tweets for these users from January and February 2015.
Limits on the number of users and tweets per user were imposed so that we could operate within Twitter's API limits.
This data supports several of our prediction tasks, as well as the four sources for each user: their tweets, tweets of mentioned
users, friends and followers.

% \input{section3_old}
%\vspace*{-5pt}
\section{User Views}
\label{sec:user_views}
%\vspace*{-5pt}
Our user dataset provides several sources of information on which we can build user views: text posted by the user ({\em ego}) and 
people that are mentioned, friended or followed by the user and their posted text.


For each text source we can aggregate the many tweets into a single document, e.g. all tweets written by accounts mentioned 
by a user.
We represent this document as a bag-of-words (\bow) in a vector space model with 
a vocabulary of the 20,000 most frequent word types after stopword removal.
We will consider both count and TF-IDF weighted vectors.


A common problem with these high dimensional representations is that they suffer from the curse of dimensionality. A natural solution is to apply a dimensionality reduction technique to find a compact representation that captures as much information as possible from the original input. Here, we consider principal components analysis (PCA), a ubiquitous linear dimensionality reduction technique, as well as word2vec~\cite{mikolov2013}, a technique to learn nonlinear word representations. 


We consider the following views for each user.
\myparagraph{\bow{}:} We take the bag-of-words (both count and TF-IDF weighted) representation of all tweets made by users in that view (ego, mention, friend, or follower) following the above pre-processing.


\myparagraph{\bowpca{}:} We run PCA and extract the top principal components for each of the above views.
We also consider \emph{all} possible combinations of views obtained by concatenating views before applying PCA,
and concatenating PCA-projected views.  By considering all possible concatenation of views, we ensure that this
method has access to the same information as multiview methods.
Both the raw \bow{} and \bowpca{} representations have been explored in previous work for demographic prediction~\cite{volkova2014,zamal2012} and recommendation systems \cite{abel2011,zangerle2013}.


\myparagraph{\wordtovec{}:} 

\bowpca{} is limited to linear representations of \bow{} features. Modern neural network based approaches to learning word embeddings, including word2vec continuous bag of words and skipgram models, can learn nonlinear representations that also capture local context around each word \cite{mikolov2013}. 
We represent each view as the simple average of the word embeddings for all tokens within that view (e.g., all words written by the ego user).
Word embeddings are learned on a sample of 87,755,398 tweets and profiles uniformly sampled from the 1\% Twitter stream in April 2015 along
with all the tweets/profiles collected for our set of users -- a total of over a billion tokens. We use the word2vec tool, select either
skipgram or continuous bag-of-words embeddings on dev data for each prediction task, and train for 50 epochs. We use the default settings for all other parameters.

\myparagraph{\collab{}:}  An alternative to text based representations is to use the social network of users as a representation. We encode a user's social network
as a vector by treating the users as a vocabulary, where users with similar social networks have similar vector representations (\collab).  An $n$-dimensional vector then encodes the user's social network as a bag-of-words over this user vocabulary. In other words, a user is represented by a summation of the one-hot encodings of each neighboring user in their social network. In this representation, the number of friends two users have in common is equal to the dot product between their social network vectors. We define the social network may be as one's followers, friends, or the union of both. 
The motivation behind this representation is that users who have similar networks may behave in similar ways. Such network features are commonly used to construct user representations as well as to make user recommendations~\cite{lu2012,kywe2012}. 

\myparagraph{\bowpcawnet{}:} The PCA-projected representations for each \collab{} vector.    This may be important for computing similarity, since users are now represented as dense vectors capturing linear correlations in the friends/followers a user has.  \bowpcawnet{} is to \collab{} as \bowpca{} is to \bow -- we apply PCA directly to the user's social network as opposed to the \bow{} representations of users in that network.


Each of these views can be treated independently as a user representation.  However, different downstream tasks may favor different 
views. For example, the friend network is useful at recommending new friends, whereas the ego tweet view may be better at predicting what content a user will post in the future.
Picking a single view may ignore valuable information as views may contain complementary information,
so using multiple views improves on a single view. One approach is to concatenate multiple views together, but this further 
increases the size of the user embeddings. In the next section, we propose an alternate approach for learning
a single embedding from multiple views.


% \input{section4_old}
%\vspace*{-5pt}
\section{Learning Multiview User Embeddings}
%\vspace*{-5pt}
\label{sec:method}
We use Generalized Canonical Correlation Analysis (\gcca{}) \cite{carroll1968generalization} to learn a single embedding
from multiple views. 
\gcca{} finds $G$,$U_i$ that minimize:
\begin{eqnarray}
\arg \min_{G,U_i} \sum_{i} \left\|G - X_i U_i \right\|_F^2 ~~~~~~
\mathrm{s.t.} \ G' G = I
\end{eqnarray}
where $X_i \in \mathbb{R}^{n \times d_i}$ corresponds to the data matrix for
the $i$th view, $U_i \in \mathbb{R}^{d_i \times k}$ maps from the latent space to observed view $i$, and $G \in \mathbb{R}^{n \times k}$  contains all user representations
\cite{van2006}.

Since each view may be more or less helpful for a downstream task, we do not want to treat each view equally in learning
a single embedding. Instead, we weigh each view differently in the objective:
\begin{eqnarray}
\arg \min_{G,U_i}\! \sum_{i}\! w_i\! \left\|G - X_i U_i \right\|_F^2 ~
\mathrm{s.t.} \ G' G = I, w_i \ge 0 \!\!
\end{eqnarray}
where $w_i$ explicitly expresses the importance of the $i$th view in determining the joint embedding.  The columns of $G$ are the eigenvectors of
$\sum_{i} w_i X_i (X_i' X_i)^{-1} X_i'$, and the solution for $U_i = (X_i' X_i)^{-1} X_i' G$.  In our experiments,
we use the approach of \newcite{rastogi2015} to learn $G$ and $U_i$, since it is more memory-efficient than decomposing the sum of projection matrices.


\gcca{} embeddings were learned over combinations of the views in \S \ref{sec:user_views}.  When available, we also consider \gccawnet{}, where in
addition to the four text views, we also include the follower and friend network views used by \bowpcawnet.
For computational efficiency, each of these views was first reduced in dimensionality
by projecting its \bow{} TF-IDF-weighted representation to a 1000-dimensional vector through PCA.\footnote{
We excluded count vectors from the \gcca{} experiments for computational efficiency since they performed
similarly to TF-IDF representations in initial experiments.
} We add
an identity matrix scaled by a small amount of regularization, $10^{-8}$, to the
per-view covariance matrices before inverting, for numerical stability, and use
the formulation of \gcca{} reported in \newcite{van2006}, which ignores rows with
missing data (some users had no data in the mention tweet view and some users accounts
were private).  We tune the weighting of each view $i$,
$w_i \in \{0.0, 0.25, 1.0\}$, discriminatively for each task, although the
\gcca{} objective is unsupervised once the $w_i$ are fixed.

We also consider a minor modification of \gcca, where $G$ is scaled by the
square-root of the singular values of $\sum_i w_i X_i X_i'$, \gccasv. 
This is inspired by previous work showing that scaling each feature
of multiview embeddings by the singular values of the data matrix can
improve performance at downstream tasks such as image or caption retrieval
\cite{mroueh2015}.  Note that if we only consider a single view, $X_1$,
with weight $w_1=1$, then the solution
to \gccasv{} is identical to the PCA solution for data matrix $X_1$, without
mean-centering.

When we compare representations in the following tasks, we sweep over embedding
width in $\{10, 20, 50, 100, 200, 300, 400, 500, 1000\}$ for all methods.
This applies to \gcca, \bowpca, \bowpcawnet, and \wordtovec.  We also consider
concatenations of vectors for every possible subset of views: singletons, pairs,
triples, and all views. We tried applying PCA directly to
the concatenation of all 1000-dimensional \bowpca{} views, but this did not
perform competitively in our experiments.



%\vspace*{-5pt}
\section{Experimental Setup}
%\vspace*{-5pt}
We selected three user prediction tasks to demonstrate the effectiveness of the multi-view embeddings: user engagement prediction,
friend recommendation and demographic characteristics inference. Our focus is to show the performance of multiview embeddings 
compared to other representations, not on building the best system for a given task.



\myparagraph{User Engagement Prediction}
The goal of user engagement prediction is to determine which topics a user will likely tweet about, using hashtag as a proxy.
This task is similar to hashtag recommendation for a tweet based on its contents
\cite{kywe2012,she2014,zangerle2013}. 
\newcite{purohit2011} presented a supervised task to predict if a hashtag would appear in a tweet using features from
the user's network, previous tweets, and the tweet's content. 

We selected the 400 most frequently used hashtags in messages authored by our users and which first appeared in March 2015,
randomly and evenly dividing them into dev and test sets.
We held out the first 10 users who tweeted each hashtag as exemplars of users that would use the hashtag in the future.
We ranked all other users by the cosine distance of their embedding to the average embedding of these 10 users.
Since embeddings are learned on data pre-March 2015, the hashtags cannot impact the learned representations.
Performance is measured using precision and recall at $k$, as well as mean reciprocal rank (MRR), where a user is marked as correct
if they used the hashtag.  Note that this task is different than that reported in~\newcite{purohit2011}, since we are making recommendations
at the level of users, not tweets.

\myparagraph{Friend Recommendation}
The goal of friend recommendation/link prediction is to recommend/predict other accounts for a user to follow \cite{liben2007}.


We selected the 500 most popular accounts -- which we call celebrities -- followed by our users, randomly, and evenly divided them into dev and test
sets.  We randomly select 10 users who follow each celebrity and rank all other users by cosine distance to the average of these 10 representations. The
tweets of selected celebrities are removed during embedding training so as not to influence the learned
representations. We use the same evaluation as user engagement prediction, where a user is marked as correct if they follow the given celebrity.

For both user engagement prediction and friend recommendation we z-score normalize each feature, subtracting off the mean
and scaling each feature independently to have unit variance, before computing cosine similarity.  We select the
approach and whether to z-score normalize based on the development set performance.

\myparagraph{Demographic Characteristics Inference}
Our final task is to infer the demographic characteristics of a user \cite{zamal2012,chen2015}.

We use the dataset from \newcite{volkova2014,volkova2015thesis}
which annotates 383 users for age (old/young), 383 for gender
(male/female), and 396 political affiliation (republican/democrat), with balanced classes.  Predicting each characteristic is a binary supervised prediction task.
Each set is partitioned into 10 folds, with two folds held out for test, and the other eight for tuning via cross-fold validation.
The provided dataset contained tweets from each user, mentioned users, friends and follower networks.  It did not contain the actual social
networks for these users, so we did not evaluate \collab, \bowpcawnet, or \gccawnet{} at these prediction tasks.

Each feature was z-score normalized before being passed to a linear-kernel SVM where we swept over $10^{-4}, \ldots, 10^{4}$ for the penalty on the error term, $C$.


%\vspace*{-5pt}
\section{Results}
\label{results}
% \vspace*{-5pt}

\myparagraph{User Engagement Prediction}


\begin{table}[t]
\begin{center}
\tiny
%\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\begin{tabular}{|l|cHH|cHH|c|c|}
\hline
\bf Model & \bf Dim & \bf P@1 & \bf P@100 & \bf P@1000 & \bf R@1 & \bf R@100 & \bf R@1000 & \bf MRR \\ \hline
\bow    & 20000 & 0.03/0.045 & 0.021/0.014 & 0.009/0.005 & 0.001/0.002 & 0.075/0.053 & 0.241/0.157 & 0.006/0.006 \\
\bowpca & 500   & \bf 0.050/0.060 & \bf 0.024/0.024 & 0.011/0.008 & 0.002/0.003 & 0.079/0.080 & 0.312/0.29 & 0.007/0.009 \\
\collab & NA    & 0.02/0.015 & 0.013/0.014 & 0.006/0.006 & 0.001/0.000 & 0.042/0.047 & 0.159/0.201 & 0.004/0.004 \\
\bowpcawnet & 300 & 0.020/0.025 & 0.019/0.019 & 0.010/0.008 & 0.001/0.002 & 0.068/0.074 & 0.293/0.299 & 0.006/0.006 \\
\wordtovec & 100 & 0.030/0.025 & 0.22/0.158 & 0.009/0.007 & 0.000/0.001 & 0.073/ 0.057 & 0.254/0.217 & 0.005/0.004 \\
\gcca  & 100 & \bf 0.080/0.060 & 0.027/0.024 & 0.012/0.009 & 0.004/0.002 & 0.095/0.093 & 0.357/0.325 & 0.011/0.008 \\
\gccasv & 500 & \bf 0.090/0.060 & 0.030/0.026 & \bf 0.012/0.010 & 0.003/0.003 & 0.104/0.106 & 0.359/0.334 & \bf 0.010/0.011 \\
\bf \gccawnet & 200 & \bf 0.065/0.060 & \bf 0.031/0.027 & 0.013/0.009 & \bf 0.003/0.004 & \bf 0.105/0.113 & \bf 0.360/0.346 & \bf 0.011/0.011 \\ \hline
\netpop & NA    & 0.000/0.000 & 0.001/0.000 & 0.001/0.001 & 0.000/0.000 & 0.001/0.002 & 0.012/0.012 & 0.000/0.000 \\
\rand   & NA    & 0.000/0.000 & 0.001/0.000 & 0.000/0.000 & 0.000/0.000 & 0.001/0.000 & 0.002/0.008 & 0.000/0.000 \\
\hline
\end{tabular}
\end{center}
%\vspace{-2ex}
\caption{ \label{tab:engagement_results} Macro performance at user engagement prediction on dev/test.
Ranking of model performance was consistent across metrics. Precision is low since few users tweet
a given hashtag. Values bolded by best test performance per metric.  Baselines (bottom): \netpop{}: a ranking of users by the size of their local network;
\rand{} randomly ranks users.}
%\vspace{-.2cm}
\end{table}

\begin{figure}[t]
\centering
\begin{tabular}{cc}
\hspace*{-25pt} \includegraphics[width=0.55\linewidth]{images/precision_hashtag_micro.pdf} & 
\hspace*{-20pt} \includegraphics[width=0.55\linewidth]{images/precision_hashtag_macro.pdf} \\ 
\hspace*{-25pt} \includegraphics[width=0.55\linewidth]{images/recall_hashtag_micro.pdf} & 
\hspace*{-20pt} \includegraphics[width=0.55\linewidth]{images/recall_hashtag_macro.pdf}
\end{tabular}

\caption{The best-performing approaches on user engagement prediction as a function of $k$ (number of
recommendations). The ordering of methods is consistent across $k$.}
\label{fig:engagement_results}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{images/view_ablation_gcca_pca.pdf}
%\vspace{-.6cm}
\caption{Macro recall@1000 on user engagement prediction for different combinations of text views.
Each bar shows the best-performing model swept over dimensionality.
\emph{E}: ego, \emph{M}: mention, \emph{Fr}: friend, \emph{Fol}: followertweet views.}
\label{fig:view_ablation}
%\vspace{-.1cm}
\end{figure}

Table \ref{tab:engagement_results} shows results for user engagement prediction and 
Figure \ref{fig:engagement_results} the precision and recall curves 
as a function of number of recommendations. 
\gcca{} outperforms other methods for precision and recall at 1000, and does close to the best in terms of MRR.
Including network views (\gccawnet{} and \gccasv) improves the performance further.
The best performing \gcca{} setting placed weight 1 on the ego tweet view, mention view,
and friend view, while \bowpca{} concatenated these views, suggesting that these were the three
most important views but that \gcca{} was able to learn a better representation.  Figure
\ref{fig:view_ablation} compares performance of different view subsets for \gcca{} and \bowpca, showing that \gcca{}
uses information from multiple views more effectively for predicting user engagement.

\myparagraph{Friend Recommendation}

\begin{table}[t]
\begin{center}
\tiny
%\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\begin{tabular}{|l|cHH|cHH|c|c|}
\hline
\bf Model & \bf Dim & \bf P@1 & \bf P@100 & \bf P@1000 & \bf R@1 & \bf R@100 & \bf R@1000 & \bf MRR \\ \hline
\bow & 20000 & 0.164/0.268 & 0.164/0.232 & 0.133/0.153 & 0.000/0.000 & 0.005/0.007 & 0.043/0.048 & 0.000/0.001 \\
\bowpca & 20 & 0.480/0.500 & 0.415/0.421 & 0.311/0.314 & 0.000/0.000 & 0.014/0.014 & 0.101/0.102 & 0.001/0.001 \\
\collab & NA    & 0.672/0.680 & 0.575/0.582 & 0.406/0.420 & 0.000/0.000 & 0.019/0.019 & 0.131/0.132 & \bf 0.002/0.002 \\
\bf \bowpcawnet & 500 & \bf 0.720/0.736 & \bf 0.596/0.601 & \bf 0.445/0.439 & 0.000/0.000 & \bf 0.020/0.020 & \bf 0.149/0.147 & \bf 0.002/0.002 \\
\wordtovec & 200 & 0.436/0.340 & 0.344/0.320 & 0.260/0.249 & 0.000/0.000 & 0.011/0.010 & 0.084/0.080 & 0.001/0.001 \\
\gcca   & 50 & 0.484/0.472 & 0.370/0.381 & 0.269/0.276 & 0.000/0.000 & 0.012/0.013 & 0.089/0.091 & 0.001/0.001 \\
\bf \gccasv   & 500 & \bf 0.720/0.736 & \bf 0.596/0.601 & \bf 0.445/0.439 & 0.000/0.000 & \bf 0.020/0.020 & \bf 0.149/0.147 & \bf 0.002/0.002 \\
\gccawnet & 20 & 0.520/0.544 & 0.481/0.475 & 0.376/0.364 & 0.000/0.000 & 0.016/0.016 & 0.123/0.120 & 0.001/0.001 \\ \hline
\netpop & NA    & 0.180/0.176 & 0.025/0.026 & 0.033/0.035 & 0.000/0.000 & 0.001/0.001 & 0.009/0.010 & 0.000/0.000 \\
\rand   & NA    & 0.072/0.136 & 0.040/0.041 & 0.034/0.036 & 0.000/0.000 & 0.001/0.001 & 0.010/0.010 & 0.000/0.000 \\
\hline
\end{tabular}
\end{center}
%\vspace{-1ex}
\caption{ \label{tab:friend_results} Macro performance for friend recommendation.  Performance of \bowpcawnet{} and \gccasv{} are
identical since the view weighting for \gccasv{} only selected solely the friend view.  Thus, these methods learned identical user embeddings.}
%\vspace{-1ex}
\end{table}

\begin{figure}[t]
\centering
\begin{tabular}{cc}
\hspace*{-25pt} \includegraphics[width=0.55\linewidth]{images/precision_friend_micro.pdf} & 
\hspace*{-20pt}\includegraphics[width=0.55\linewidth]{images/precision_friend_macro.pdf} \\ 
\hspace*{-25pt} \includegraphics[width=0.55\linewidth]{images/recall_friend_micro.pdf} & 
\hspace*{-20pt} \includegraphics[width=0.55\linewidth]{images/recall_friend_macro.pdf}
\end{tabular}
%\vspace{-.2cm}
\caption{Performance on friend recommendation varying $k$.}
\label{fig:friend_results}
%\vspace{-.4cm}
\end{figure}


\begin{table}[t!] 
\begin{center}
\tiny
\begin{tabular}{|l|c|c|c|}
\hline
\bf Model & \bf age & \bf gender & \bf politics \\ \hline
\bow  & 0.771/0.740 & 0.723/0.662 & 0.934/0.975 \\
\bowpca & 0.784/0.649 & 0.719/0.662 & 0.908/0.900 \\
\bowpcaandbow & 0.767/0.688 & 0.660/0.714 & \bf 0.937/0.9875 \\
\gcca   & 0.725/0.740 & 0.742/0.714 & 0.899/0.8125 \\
\gccaandbow & 0.764/0.727 & 0.657/0.701 & 0.940/0.9625 \\
\gccasv     & 0.709/0.636 & 0.699/0.714 & 0.871/0.850 \\
\gccasvandbow & 0.761/0.688 & 0.647/0.675 & 0.937/0.9625 \\
\wordtovec & \bf 0.790/0.753 & \bf 0.777/0.766 & 0.927/0.938 \\ \hline
\end{tabular}
\end{center}
%\vspace{-1ex}
\caption{ \label{tab:demographic_results} Average CV/test accuracy for inferring demographic characteristics.}
%\vspace{-1.5ex}
\end{table}


Table \ref{tab:friend_results} shows results for friend prediction and Figure \ref{fig:friend_results} similarly shows that performance differences
between approaches are consistent across $k$ (number of recommendations.)
Adding network views to \gcca, \gccawnet, improves performance, although it cannot contend with \collab{} or \bowpcawnet, although \gccasv{} is able to meet the performance of \bowpcawnet.
The best \gcca{} placed non-zero weight on the friend tweets view, and \gccawnet{} only
places weight on the friend network view; the other views were not
informative.  \bowpca{} and \wordtovec{} only used the friend tweet view.
This suggests that the friend view is the most important for this task, and multiview techniques cannot exploit additional views to improve performance.
\gccasv{} performs identically to \gccawnet, since it only placed weight on the friend network view, learning identical embeddings to \gccawnet.


\myparagraph{Demographic Characteristics Prediction}

Table \ref{tab:demographic_results} shows the average cross-fold validation and
test accuracy on the demographic prediction task.  \gccaandbow{} and \bowpcaandbow{} are the concatenation of \bow{}
features with \gcca{} and \bowpca{}, respectively.  The wide variation in performance is due to the small size of the datasets,
thus it's hard to draw many conclusions other than that \gcca{} seems to perform well compared to other linear methods.
\wordtovec{} surpasses other representations in two out of three datasets.

It is difficult to compare the performance of the methods we evaluate here to that reported in previous work,
\cite{zamal2012}.  This is because they report cross-fold validation accuracy (not test),
they consider a wider range of hand-engineered features, different subsets of networks, radial basis function kernels for SVM, and find
that accuracy varies wildly across different feature sets.  They report cross-fold validation accuracy ranging from 0.619 to 0.805 for
predicting age, 0.560 to 0.802 for gender, and 0.725 to 0.932 for politics.

\section{Conclusion}
\label{discussion}

We have proposed several representations of Twitter users, as well as a multiview approach that combines these views into
a single embedding. Our multiview embeddings achieve promising results on three different prediction tasks, making use of both
what a user writes as well as the social network.  We found that each task relied on different information, which our method 
successfully combined into a single representation.

We plan to consider other means for learning user representations, including comparing nonlinear dimensionality reduction techniques such as kernel PCA \cite{scholkopf1997} and deep canonical correlation analysis \cite{andrew2013,wang2015deep}.
Recent work on learning user representations with multitask deep learning techniques \cite{li2015}, suggests that learning a nonlinear mapping from observed views to the latent space can learn high quality user representations.
One issue with \gcca{} is scalability: solving for $G$ relies on an SVD of a large matrix that must be loaded into memory.  Online variants of \gcca{} would allow this method to scale to larger training sets and incrementally update representations.  The PCA-reduced views for all 102,328 Twitter users can be found here: \url{http://www.dredze.com/datasets/multiview_embeddings/}.

\subsubsection*{Acknowledgements}
\noindent This research was supported in part by NSF BIGDATA grant IIS-1546482 and a gift
from Bloomberg LP. 

\begin{thebibliography}{}

\bibitem[\protect\citename{Abel \bgroup et al.\egroup }2011]{abel2011}
Fabian Abel, Qi~Gao, Geert{-}Jan Houben, and Ke~Tao.
\newblock 2011.
\newblock Analyzing user modeling on twitter for personalized news
  recommendations.
\newblock In {\em User Modeling, Adaption and Personalization - 19th
  International Conference, {UMAP} 2011, Girona, Spain, July 11-15, 2011.
  Proceedings}, pages 1--12.

\bibitem[\protect\citename{Al~Zamal \bgroup et al.\egroup }2012]{zamal2012}
Faiyaz Al~Zamal, Wendy Liu, and Derek Ruths.
\newblock 2012.
\newblock Homophily and latent attribute inference: Inferring latent attributes
  of twitter users from neighbors.
\newblock In {\em Internation Conference on Weblogs and Social Media (ICWSM)}.

\bibitem[\protect\citename{Andrew \bgroup et al.\egroup }2013]{andrew2013}
Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu.
\newblock 2013.
\newblock Deep canonical correlation analysis.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  1247--1255.

\bibitem[\protect\citename{Arora and Livescu}2014]{arora2014multi}
Raman Arora and Karen Livescu.
\newblock 2014.
\newblock Multi-view learning with supervision for transformed bottleneck
  features.
\newblock In {\em Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE
  International Conference on}, pages 2499--2503. IEEE.

\bibitem[\protect\citename{Carroll}1968]{carroll1968generalization}
J~Douglas Carroll.
\newblock 1968.
\newblock Generalization of canonical correlation analysis to three or more
  sets of variables.
\newblock In {\em Convention of the American Psychological Association},
  volume~3, pages 227--228.

\bibitem[\protect\citename{Chen \bgroup et al.\egroup }2015]{chen2015}
Xin Chen, Yu~Wang, Eugene Agichtein, and Fusheng Wang.
\newblock 2015.
\newblock A comparative study of demographic attribute inference in twitter.
\newblock In {\em Conference on Weblogs and Social Media (ICWSM)}.

\bibitem[\protect\citename{Faruqui and Dyer}2014]{faruqui2014}
Manaal Faruqui and Chris Dyer.
\newblock 2014.
\newblock Improving vector space word representations using multilingual
  correlation.
\newblock In {\em Proceedings of the 14th Conference of the European Chapter of
  the Association for Computational Linguistics, {EACL} 2014, April 26-30,
  2014, Gothenburg, Sweden}, pages 462--471.

\bibitem[\protect\citename{Kiros \bgroup et al.\egroup }2015]{kiros2015}
Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard~S. Zemel, Antonio
  Torralba, Raquel Urtasun, and Sanja Fidler.
\newblock 2015.
\newblock Skip-thought vectors.
\newblock {\em CoRR}, abs/1506.06726.

\bibitem[\protect\citename{Kywe \bgroup et al.\egroup }2012]{kywe2012}
Su~Mon Kywe, Tuan{-}Anh Hoang, Ee{-}Peng Lim, and Feida Zhu.
\newblock 2012.
\newblock On recommending hashtags in twitter networks.
\newblock In {\em Social Informatics - 4th International Conference, SocInfo
  2012, Lausanne, Switzerland, December 5-7, 2012. Proceedings}, pages
  337--350.

\bibitem[\protect\citename{Le and Mikolov}2014]{le2014}
Quoc~V Le and Tomas Mikolov.
\newblock 2014.
\newblock Distributed representations of sentences and documents.
\newblock In {\em Internation Conference on Machine Learning (ICML)}.

\bibitem[\protect\citename{Li \bgroup et al.\egroup }2015]{li2015}
Jiwei Li, Alan Ritter, and Dan Jurafsky.
\newblock 2015.
\newblock Learning multi-faceted representations of individuals from
  heterogeneous evidence using neural networks.
\newblock {\em arXiv preprint arXiv:1510.05198}.

\bibitem[\protect\citename{Liben-Nowell and Kleinberg}2007]{liben2007}
David Liben-Nowell and Jon Kleinberg.
\newblock 2007.
\newblock The link-prediction problem for social networks.
\newblock {\em Journal of the American society for information science and
  technology}, 58(7):1019--1031.

\bibitem[\protect\citename{Lu \bgroup et al.\egroup }2012]{lu2012}
Chunliang Lu, Wai Lam, and Yingxiao Zhang.
\newblock 2012.
\newblock Twitter user modeling and tweets recommendation based on wikipedia
  concept graph.
\newblock In {\em IJCAI Workshop on Intelligent Techniques for Web
  Personalization and Recommender Systems}.

\bibitem[\protect\citename{Lui and Baldwin}2012]{lui2012langid}
Marco Lui and Timothy Baldwin.
\newblock 2012.
\newblock langid. py: An off-the-shelf language identification tool.
\newblock In {\em Association for Computational Linguistics (ACL): system
  demonstrations}, pages 25--30.

\bibitem[\protect\citename{Mikolov \bgroup et al.\egroup }2013]{mikolov2013}
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg~S Corrado, and Jeff Dean.
\newblock 2013.
\newblock Distributed representations of words and phrases and their
  compositionality.
\newblock In {\em Advances in neural information processing systems (NIPS)},
  pages 3111--3119.

\bibitem[\protect\citename{Mroueh \bgroup et al.\egroup }2015]{mroueh2015}
Youssef Mroueh, Etienne Marcheret, and Vaibhava Goel.
\newblock 2015.
\newblock Asymmetrically weighted cca and hierarchical kernel sentence
  embedding for multimodal retrieval.
\newblock {\em arXiv preprint arXiv:1511.06267}.

\bibitem[\protect\citename{Purohit \bgroup et al.\egroup }2011]{purohit2011}
Hemant Purohit, Yiye Ruan, Amruta Joshi, Srinivasan Parthasarathy, and Amit
  Sheth.
\newblock 2011.
\newblock Understanding user-community engagement by multi-faceted features: A
  case study on twitter.
\newblock In {\em WWW Workshop on Social Media Engagement (SoME)}.

\bibitem[\protect\citename{Rastogi \bgroup et al.\egroup }2015]{rastogi2015}
Pushpendre Rastogi, Benjamin Van~Durme, and Raman Arora.
\newblock 2015.
\newblock Multiview lsa: Representation learning via generalized cca.
\newblock In {\em North American Association for Computational Linguistics
  (NAACL)}.

\bibitem[\protect\citename{Sch{\"{o}}lkopf \bgroup et al.\egroup
  }1997]{scholkopf1997}
Bernhard Sch{\"{o}}lkopf, Alexander~J. Smola, and Klaus{-}Robert M{\"{u}}ller.
\newblock 1997.
\newblock Kernel principal component analysis.
\newblock In {\em Artificial Neural Networks - {ICANN} '97, 7th International
  Conference, Lausanne, Switzerland, October 8-10, 1997, Proceedings}, pages
  583--588.

\bibitem[\protect\citename{She and Chen}2014]{she2014}
Jieying She and Lei Chen.
\newblock 2014.
\newblock Tomoha: Topic model-based hashtag recommendation on twitter.
\newblock In {\em International conference on World wide web (WWW)}, pages
  371--372. International World Wide Web Conferences Steering Committee.

\bibitem[\protect\citename{Van De~Velden and Bijmolt}2006]{van2006}
Michel Van De~Velden and Tammo~HA Bijmolt.
\newblock 2006.
\newblock Generalized canonical correlation analysis of matrices with missing
  rows: a simulation study.
\newblock {\em Psychometrika}, 71(2):323--331.

\bibitem[\protect\citename{Volkova \bgroup et al.\egroup }2014]{volkova2014}
Svitlana Volkova, Glen Coppersmith, and Benjamin Van~Durme.
\newblock 2014.
\newblock Inferring user political preferences from streaming communications.
\newblock In {\em Association for Computational Linguistics (ACL)}, pages
  186--196.

\bibitem[\protect\citename{Volkova}2015]{volkova2015thesis}
Svitlana Volkova.
\newblock 2015.
\newblock {\em Predicting Demographics and Affect in Social Networks}.
\newblock {Ph.D.} thesis, Johns Hopkins University, October.

\bibitem[\protect\citename{Wang \bgroup et al.\egroup }2015]{wang2015deep}
Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes.
\newblock 2015.
\newblock On deep multi-view representation learning.
\newblock In {\em Proceedings of the 32nd International Conference on Machine
  Learning (ICML-15)}, pages 1083--1092.

\bibitem[\protect\citename{Zangerle \bgroup et al.\egroup }2013]{zangerle2013}
Eva Zangerle, Wolfgang Gassler, and G{\"u}nther Specht.
\newblock 2013.
\newblock On the impact of text similarity functions on hashtag recommendations
  in microblogging environments.
\newblock {\em Social Network Analysis and Mining}, 3(4):889--898.

\end{thebibliography}

\bibliographystyle{acl2016}

\end{document} 

