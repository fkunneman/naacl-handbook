SubmissionNumber#=%=#85
FinalPaperTitle#=%=#Vocabulary Manipulation for Neural Machine Translation
ShortPaperTitle#=%=#Vocabulary Manipulation for Neural Machine Translation
NumberOfPages#=%=#6
CopyrightSigned#=%=#Haitao Mi
JobTitle#==#
Organization#==#
Abstract#==#In order to capture rich language phenomena, neural machine translation models
have to use a large vocabulary size, which requires high computing time and
large memory usage. In this paper, we alleviate this issue by introducing a
sentence-level or batch-level vocabulary, which is only a very small sub-set of

the full output vocabulary. For each sentence or batch, we only predict the
target words in its sentence-level or batch-level vocabulary. Thus, we reduce
both the computing time and the memory usage. Our method simply takes into
account the translation options of each word or phrase in the source sentence,
and picks a very small target vocabulary for each sentence %or mini-batch 
based on a word-to-word translation model or a bilingual phrase library learned

from a traditional machine translation model. Experimental results on the
large-scale English-to-French task show that our method achieves better
translation performance by 1 BLEU point over the large vocabulary neural
machine translation system of Jean et al. (2015).
Author{1}{Firstname}#=%=#Haitao
Author{1}{Lastname}#=%=#Mi
Author{1}{Email}#=%=#hmi@us.ibm.com
Author{1}{Affiliation}#=%=#IBM Watson Research Center
Author{2}{Firstname}#=%=#Zhiguo
Author{2}{Lastname}#=%=#Wang
Author{2}{Email}#=%=#zgw.tomorrow@gmail.com
Author{2}{Affiliation}#=%=#IBM Watson Research Center
Author{3}{Firstname}#=%=#Abe
Author{3}{Lastname}#=%=#Ittycheriah
Author{3}{Email}#=%=#abei@us.ibm.com
Author{3}{Affiliation}#=%=#IBM

==========