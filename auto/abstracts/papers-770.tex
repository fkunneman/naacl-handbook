Precise evaluation metrics are important for assessing progress in high-level language generation tasks such as machine translation or image captioning. Historically, these metrics have been evaluated using correlation with human judgment, however human-derived scores are often alarmingly inconsistent and also limited in their ability to identify precise areas of weakness. In this paper, we perform a case study for metric evaluation based on known sentence corruptions which serve as comprehensive unit tests for comparing metrics. We analyze three broad categories of sentence transformations and evaluate how successfully several common metrics can detect them. We find that not only are human annotations heavily inconsistent in this study, but that the Metric Unit TesT analysis is better able to capture precise shortcomings of particular metrics (e.g. comparing passive and active sentences) than a simple correlation with human judgment would.
