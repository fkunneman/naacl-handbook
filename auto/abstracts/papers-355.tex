Most sentence embedding models typically represent each sentence only using word surface, which makes these models indiscriminative for ubiquitous polysemy. In order to enhance semantic representation capability of sentence, we employ text conceptualization algorithm to assign associated concepts for each sentence, and then learn conceptual sentence embedding (CSE). Therefore, this concept-level representation is more expressive than some widely-used text representation models such as latent topic model, especially for short-text. Moreover, we further extend CSE models by utilizing a local attention-based model that select relevant words within the context to make more efficient prediction. The experimental results on text classification task and information retrieval task, show that the proposed models outperform the topic model-based sentence representation methods and other sentence embedding models.
