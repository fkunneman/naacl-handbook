We submitted four systems for task 1 and two systems for task 2. The multimodal neural systems are a joint submission from LIUM and CVC (LIUMCVC). The phrase-based and monomodal NMT system have been submitted by LIUM. The phrase-based systems are based on Moses (14 standard features + OSM). They include rescoring with several models and more particularly with a continuous space language model integrating visual features as auxiliary data. We also developed a multimodal neural machine translation system with a shared attention mechanism. The text input is encoded by a bidirectional recurrent layer. The image is encoded by a very deep residual network (up to 152 layers). The attention weights are computed for each input word and each image feature map by a single fully-connected feed-forward network. The decoder is build upon a conditional gated recurrent unit (cGRU) and generates its output based on the multimodal input context (with attention), the previous output and the hidden state of the cGRU. All our systems are constrained
