Broad-domain question answering often requires shallow textual methods in order to gain coverage beyond facts that are available in structured knowledge bases, but simultaneously can benefit from the high precision of logical reasoning. We propose an approach for incorporating both of these signals in a unified framework based on natural logic. We train an evaluation function --- akin to gameplaying --- to evaluate the expected truth of candidate premises on the fly. As an added contribution, we extend the breadth of inferences afforded by natural logic to include relational entailment (e.g., buy --> own) and meronymy (e.g., a person born in a city is born in the city's country). We evaluate our approach on answering multiple-choice science questions, outperforming prior work on the dataset.
