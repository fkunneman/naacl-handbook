Research at the intersection of language and other modalities, most notably vision, is becoming increasingly important in our field. We introduce a toolkit that can be used to obtain feature representations for visual as well as auditory information. MMFEAT is an easy-to-use Python toolkit, which has been developed with the purpose of making non-linguistic modalities more accessible to natural language processing researchers.
