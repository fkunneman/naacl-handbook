Linguistic annotation is the backbone behind any successful approach in Natural Language Processing (NLP), because annotated corpora are necessary to train and evaluate natural language processing applications. Thus, a lot of effort is put into obtaining high-agreement annotated datasets. Recent research has shown that annotator disagreement carries actual signal for regularizing NLP models. However, prior work focused on part of speech tagging and on a single language. We take this further by examining a semantic task (supersense tagging) and the cross-language scenario. In particular, we examine how reliable disagreement is for sense annotation and we gauge in a preliminary study whether disagreements transfer across languages.
