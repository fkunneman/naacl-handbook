Word representations are a fundamental component for many natural language processing tasks. In this paper, we propose a generic energy-based model, MsVec, which is capable of learning word representations from multiple information sources. We instantiate the proposed generic model as dep-MsVec by emphasizing sentences' syntactic dependency information. Experimental results show a boost in word analogy tasks, especially in syntactic aspects, as expected.
