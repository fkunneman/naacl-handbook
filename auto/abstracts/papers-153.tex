The effort of understanding the meaning of words is central to the NLP community.  Coarse-grained semantic categories such as supersenses were proven useful for a range of downstream tasks such as question answering or machine translation. To date, however, no effort has been put into integrating the supersenses into distributional word representations. In this work, we present a novel joint embedding model of words and supersenses, providing insights into the relationship between words and supersenses in the same vector space. Using these embeddings in a deep neural network model, we demonstrate that the supersense enrichment leads to a significant improvement in a range of downstream classification tasks. We also manifest the utility of these embeddings for predicting supersenses, achieving competitive results.
