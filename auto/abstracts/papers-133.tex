Distant supervised relation extraction has been widely used in finding novel relational facts from text. However, distant supervision inevitably accompanies with the wrong labelling problem, and these noisy data will substantially hurt the performance of relation extraction. To alleviate this issue, we propose an attention-based model for relation extraction. In this model, we employ convolutional neural networks to embed the semantics of sentences. Afterwards, we build sentence-level attention over multiple instances, which is expected to dynamically reduce the weights of those noisy instances during learning. Experimental results on real-world datasets show that, our model can effectively reduce the influence of wrong labelling instances and achieves significant and consistent improvements on relation extraction as compared with baselines.
