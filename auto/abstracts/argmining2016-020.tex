Argument mining integrates many distinct computational linguistics tasks, and as a result, reporting agreement between annotators or between automated output and gold standard is particularly challenging. More worrying  for the field, agreement and performance are also reported in a wide variety of different ways, making comparison between approaches difficult. To solve this  problem, we propose the CASS technique for combining metrics covering different parts of the argument mining task. CASS delivers a justified method of integrating results yielding confusion  matrices from which CASS-k and CASS-F1  scores can be calculated.
