We describe a system to collect web data for Low Resource Languages, to augment language model training data for Automatic Speech Recognition (ASR) and keyword search by reducing the Out-of-Vocabulary (OOV) rates -- words in the test set that did not appear in the training set for ASR.   We test this system on seven Low Resource Languages from the IARPA Babel Program:  Paraguayan Guarani, Igbo, Amharic, Halh Mongolian, Javanese, Pashto, and Dholuo.              The success of our system compared with other web collection systems is due to the targeted collection sources (blogs, twitter, forums) and the  inclusion of a separate language identification component in its pipeline, which filters the data initially collected before finally saving it.  Our results show a major reduction of OOV rates relative to those calculated from training corpora alone and major reductions in OOV rates calculated in terms of keywords in the training development set.  We also describe differences among genres in this reduction, which vary by language but show a pronounced influence for augmentation from Twitter data for most languages.
